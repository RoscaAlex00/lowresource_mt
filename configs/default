model:
  name: "SI2M-Lab/DarijaBERT"
  decoder_name: "Helsinki-NLP/opus-mt-en"
  max_length: 128

tokenizer:
  encoder_max_length: 128
  decoder_max_length: 128

trainer:
  batch_size: 16  # Batch size for training
  num_epochs: 3  # Number of epochs to train for
  warmup_steps: 500  # Warmup steps for learning rate scheduler
  gradient_accumulation_steps: 1  # Gradient accumulation steps
  fp16: true  # Enable mixed precision training (if you have a compatible GPU)
  save_model: true  # Whether to save the model after training
  predict: true  # Whether to predict on the test set
  eval: true  # Whether to evaluate on validation set

generate:
  max_length: 128  # Maximum length for generated sequences
  min_length: 10  # Minimum length for generated sequences
  num_beams: 5  # Number of beams for beam search
  early_stopping: true  # Stop when all beams reach the end token
  no_repeat_ngram_size: 3  # Prevent repetitions of n-grams

seed: 42  # Random seed for reproducibility

data:
  train:
  validation:
  test:
